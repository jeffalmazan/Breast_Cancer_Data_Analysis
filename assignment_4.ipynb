{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading the Breast Cancer dataset...\n",
      "[INFO] Dataset overview:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "Dataset shape: (569, 31)\n",
      "\n",
      "Target variable counts:\n",
      " target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the Breast Cancer dataset\n",
    "print(\"[INFO] Loading the Breast Cancer dataset...\")\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame for easier handling\n",
    "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  # Add the target column\n",
    "\n",
    "# Display dataset overview\n",
    "print(\"[INFO] Dataset overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nTarget variable counts:\\n\", df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Handle missing values (if any)\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum().sum()\n",
    "print(\"\\nMissing values in the dataset:\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Splitting the dataset into training and testing sets...\n",
      "Training set shape: (455, 30)\n",
      "Testing set shape: (114, 30)\n",
      "\n",
      "[INFO] Normalizing the feature data...\n",
      "\n",
      "[INFO] Data preparation complete. The dataset is ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Split data into features (X) and target (y)\n",
    "X = df.drop('target', axis=1)  # Features\n",
    "y = df['target']  # Target variable\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "print(\"\\n[INFO] Splitting the dataset into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "\n",
    "# Step 5: Normalize/Scale the feature data\n",
    "print(\"\\n[INFO] Normalizing the feature data...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform on training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Only transform on testing data\n",
    "\n",
    "# Convert scaled data back to DataFrame for easy handling (optional)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "print(\"\\n[INFO] Data preparation complete. The dataset is ready for analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Selected Features:\n",
      " Index(['mean radius', 'mean perimeter', 'mean area', 'mean concavity',\n",
      "       'mean concave points', 'worst radius', 'worst perimeter', 'worst area',\n",
      "       'worst concavity', 'worst concave points'],\n",
      "      dtype='object')\n",
      "[INFO] Dataset split into training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature Selection using SelectKBest\n",
    "selector = SelectKBest(score_func=f_classif, k=10)  # Selecting the top 10 features\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"[INFO] Selected Features:\\n\", selected_features)\n",
    "\n",
    "# Split the dataset into training and testing sets with selected features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"[INFO] Dataset split into training and testing sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running GridSearchCV...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[INFO] Best Parameters:\n",
      " {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "[INFO] Best Score:\n",
      " 0.9340659340659341\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the base MLPClassifier model\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],  # Different layer configurations\n",
    "    'activation': ['relu', 'tanh'],  # Activation functions\n",
    "    'solver': ['adam', 'sgd'],  # Optimizers\n",
    "    'alpha': [0.0001, 0.001],  # Regularization term\n",
    "    'learning_rate': ['constant', 'adaptive'],  # Learning rate types\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV on training data\n",
    "print(\"[INFO] Running GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and best score\n",
    "print(\"[INFO] Best Parameters:\\n\", grid_search.best_params_)\n",
    "print(\"[INFO] Best Score:\\n\", grid_search.best_score_)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training the ANN model with best parameters...\n",
      "[INFO] Evaluation Results:\n",
      "Test Accuracy: 0.8508771929824561\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82        42\n",
      "           1       0.94      0.82      0.87        72\n",
      "\n",
      "    accuracy                           0.85       114\n",
      "   macro avg       0.84      0.86      0.85       114\n",
      "weighted avg       0.87      0.85      0.85       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train the best model\n",
    "print(\"[INFO] Training the ANN model with best parameters...\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(\"[INFO] Evaluation Results:\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (e:\\Lambton Files\\Term 3\\Gupta Neural Network\\Week 13\\.venv\\Lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SelectKBest, f_classif\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_breast_cancer\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m joblib\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the Breast Cancer dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m load_breast_cancer()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (e:\\Lambton Files\\Term 3\\Gupta Neural Network\\Week 13\\.venv\\Lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Feature Selection\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "# Load the pre-trained ANN model\n",
    "model = joblib.load(\"best_ann_model.pkl\")  # Save your model as 'best_ann_model.pkl'\n",
    "\n",
    "# Streamlit App\n",
    "st.title(\"Breast Cancer Prediction App\")\n",
    "st.write(\"This app predicts whether breast cancer is malignant or benign based on user inputs.\")\n",
    "\n",
    "# Sidebar for user input\n",
    "st.sidebar.header(\"Input Features\")\n",
    "\n",
    "def user_input_features():\n",
    "    input_data = {}\n",
    "    for feature in selected_features:\n",
    "        input_data[feature] = st.sidebar.slider(feature, float(X[feature].min()), float(X[feature].max()), float(X[feature].mean()))\n",
    "    return pd.DataFrame([input_data])\n",
    "\n",
    "user_data = user_input_features()\n",
    "st.write(\"### User Input Features\", user_data)\n",
    "\n",
    "# Predict and Display Results\n",
    "if st.button(\"Predict\"):\n",
    "    prediction = model.predict(user_data)\n",
    "    prediction_proba = model.predict_proba(user_data)\n",
    "    st.write(\"### Prediction:\", \"Malignant\" if prediction[0] == 0 else \"Benign\")\n",
    "    st.write(\"### Prediction Probability:\", prediction_proba)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
